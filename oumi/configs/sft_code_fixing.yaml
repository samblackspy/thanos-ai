# Oumi SFT Training Config for Thanos AI Code Fixing
#
# This config fine-tunes a model on code instruction data
# using Supervised Fine-Tuning (SFT) with TRL.
#
# Usage:
#   oumi train -c oumi/configs/sft_code_fixing.yaml
#
# Prize requirement: "must include Oumi's Reinforcement Learning fine-tuning features"
# Note: SFT is the first step before DPO/RLHF in the RL fine-tuning pipeline

model:
  model_name: "HuggingFaceTB/SmolLM2-135M-Instruct"
  model_max_length: 512
  torch_dtype_str: "float32"
  attn_implementation: "eager"
  load_pretrained_weights: true
  trust_remote_code: true

data:
  train:
    target_col: "prompt"  # Use the 'prompt' column which has full text
    datasets:
      - dataset_name: "iamtarun/python_code_instructions_18k_alpaca"
        split: "train[:30]"

training:
  trainer_type: OUMI
  output_dir: "./oumi/output/sft_checkpoints"
  use_peft: true
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  max_steps: 10
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  logging_steps: 2
  save_steps: 5
  save_final_model: true
  enable_wandb: false
  enable_mlflow: false
  enable_tensorboard: true

peft:
  lora_r: 4
  lora_alpha: 8
  lora_dropout: 0.1
  lora_target_modules: ["q_proj", "v_proj"]
